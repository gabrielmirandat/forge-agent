"""Integration test for LangChain executor with Ollama and MCP tools.

Tests the complete flow:
1. LangChain executor initialization
2. Ollama LLM connection
3. MCP tools availability
4. Agent execution with tool calling
5. File creation via filesystem tool
"""

import asyncio
import os
import tempfile
from pathlib import Path

import pytest

from agent.config.loader import AgentConfig, ConfigLoader
from agent.runtime.langchain_executor import LangChainExecutor
from agent.tools.base import ToolRegistry


@pytest.fixture
def test_workspace():
    """Create a temporary workspace for testing."""
    with tempfile.TemporaryDirectory() as tmpdir:
        yield tmpdir


@pytest.fixture
def test_config(test_workspace):
    """Create test configuration."""
    config_path = Path(__file__).parent.parent.parent / "config" / "agent.ollama.yaml"
    loader = ConfigLoader(config_path=str(config_path))
    config = loader.load()
    
    # Override workspace path for testing
    config.workspace.base_path = test_workspace
    
    return config


@pytest.fixture
def tool_registry(test_config):
    """Initialize tool registry with MCP tools."""
    import asyncio
    import nest_asyncio
    from api.dependencies import _register_mcp_tools
    
    # Apply nest_asyncio to allow nested event loops
    nest_asyncio.apply()
    
    registry = ToolRegistry()
    
    # Register MCP tools (run async code in sync fixture)
    try:
        loop = asyncio.get_event_loop()
        if loop.is_running():
            # If loop is already running, use nest_asyncio
            asyncio.run(_register_mcp_tools(registry, test_config))
        else:
            loop.run_until_complete(_register_mcp_tools(registry, test_config))
    except RuntimeError:
        # No event loop, create one
        asyncio.run(_register_mcp_tools(registry, test_config))
    
    return registry


@pytest.mark.asyncio
async def test_langchain_create_file(test_config, tool_registry, test_workspace):
    """Test LangChain executor creating a file via tool calling.
    
    This test verifies:
    1. LangChain executor can be initialized
    2. Ollama LLM can be connected
    3. MCP tools are available
    4. Agent can execute tool calls
    5. File is created successfully
    """
    # Change to test workspace
    original_cwd = os.getcwd()
    try:
        os.chdir(test_workspace)
        
        # Initialize LangChain executor
        executor = LangChainExecutor(
            config=test_config,
            tool_registry=tool_registry,
            session_id="test_session",
            max_iterations=10,
        )
        
        # Verify executor is initialized (agent graph will be created lazily on first run)
        assert executor is not None
        
        # Execute prompt to create hello-world.py
        # Use a more explicit prompt that forces tool usage
        prompt = "Use the write_file tool to create a file named hello-world.py in the current directory with the content: print('Hello, World!')"
        
        print(f"\nðŸ” Executing prompt: {prompt}")
        print(f"ðŸ“ Working directory: {test_workspace}")
        print(f"ðŸ”§ Available tools: {[tool.name for tool in executor.langchain_tools if 'write' in tool.name.lower() or 'file' in tool.name.lower()]}")
        
        result = await executor.run(
            user_message=prompt,
            conversation_history=None,
        )
        
        # Verify execution succeeded
        assert result is not None, "Result is None"
        print(f"\nðŸ“Š Execution result: {result}")
        
        if not result.get("success"):
            error_msg = result.get("error", "Unknown error")
            print(f"âŒ Execution failed: {error_msg}")
            print(f"ðŸ“ Response: {result.get('response', 'No response')}")
        
        assert result.get("success") is True, f"Execution failed: {result.get('error', 'Unknown error')}. Response: {result.get('response', 'No response')}"
        
        # Verify file was created - check for any .py file with hello world content
        # The LLM might create a file with a different name
        python_files = list(Path(test_workspace).glob("*.py"))
        if not python_files:
            # List all files in workspace for debugging
            all_files = list(Path(test_workspace).iterdir())
            print(f"\nðŸ“ Files in workspace: {[f.name for f in all_files]}")
            print(f"ðŸ“ Agent response: {result.get('response', 'No response')}")
            print(f"ðŸ”§ Tools called: {result.get('tools', {}).get('calls', [])}")
        
        # Check if any Python file was created with hello world content
        file_created = False
        created_file = None
        for py_file in python_files:
            content = py_file.read_text()
            if "hello" in content.lower() or "Hello" in content or "print" in content:
                file_created = True
                created_file = py_file
                break
        
        # Also check the expected file name
        file_path = Path(test_workspace) / "hello-world.py"
        if file_path.exists():
            file_created = True
            created_file = file_path
        
        # Check if tool was called
        tool_was_called = result.get("tools", {}).get("calls_count", 0) > 0
        
        if not file_created:
            # If tool was called but file wasn't created, that's a real problem
            if tool_was_called:
                pytest.fail(f"Tool was called but file was not created. Files in workspace: {[f.name for f in Path(test_workspace).iterdir()]}. Response: {result.get('response', 'No response')[:200]}")
            else:
                # LLM didn't use the tool - this is a model behavior issue, not a code issue
                pytest.skip(f"LLM did not use write_file tool (tool_calls_count=0). This is a model behavior issue, not a code bug. Response: {result.get('response', 'No response')[:200]}")
        
        # Verify file content
        content = created_file.read_text()
        assert "Hello" in content or "hello" in content.lower() or "print" in content, f"Unexpected file content: {content}"
        
        print(f"âœ… File created successfully: {created_file}")
        print(f"ðŸ“„ File content:\n{content}")
        
    finally:
        os.chdir(original_cwd)


@pytest.mark.asyncio
async def test_langchain_simple_prompt(test_config, tool_registry):
    """Test LangChain executor with a simple prompt (no tools).
    
    This test verifies basic LLM communication without tool calling.
    """
    # Initialize LangChain executor
    executor = LangChainExecutor(
        config=test_config,
        tool_registry=tool_registry,
        session_id="test_session_simple",
        max_iterations=5,
    )
    
    # Simple prompt that doesn't require tools
    prompt = "What is 2 + 2? Answer with just the number."
    
    print(f"\nðŸ” Executing simple prompt: {prompt}")
    
    result = await executor.run(
        user_message=prompt,
        conversation_history=None,
    )
    
    # Verify execution succeeded
    assert result is not None
    assert result.get("success") is True, f"Execution failed: {result.get('error', 'Unknown error')}"
    
    response = result.get("response", "")
    assert response is not None
    assert len(response) > 0
    
    print(f"âœ… LLM response: {response}")
