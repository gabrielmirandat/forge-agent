version: '3.8'

services:
  ollama:
    image: ollama/ollama:latest
    container_name: phase1-ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    environment:
      - OLLAMA_HOST=0.0.0.0
    restart: unless-stopped
    networks:
      - phase1-network

  # Optional: LocalAI alternative
  localai:
    image: localai/localai:latest-aio-cuda
    container_name: phase1-localai
    ports:
      - "8080:8080"
    volumes:
      - localai-data:/build/models
      - ./configs:/configs
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    environment:
      - MODELS_PATH=/build/models
      - CONTEXT_SIZE=4096
    restart: unless-stopped
    networks:
      - phase1-network
    profiles:
      - localai  # Only start with: docker-compose --profile localai up

  # Evaluation tool container
  evaluator:
    build:
      context: .
      dockerfile: Dockerfile.evaluator
    container_name: phase1-evaluator
    volumes:
      - ./outputs:/app/results
      - ./evaluate_model.py:/app/evaluate_model.py
      - ./validate_output.py:/app/validate_output.py
    environment:
      - OLLAMA_HOST=http://ollama:11434
      - LOCALAI_HOST=http://localai:8080
    depends_on:
      - ollama
    networks:
      - phase1-network
    profiles:
      - evaluator  # Only start when needed
    command: tail -f /dev/null  # Keep container running

volumes:
  ollama-data:
    driver: local
  localai-data:
    driver: local

networks:
  phase1-network:
    driver: bridge

