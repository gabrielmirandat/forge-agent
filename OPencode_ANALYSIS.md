# An√°lise do OpenCode vs Forge Agent

## üìã Vis√£o Geral

### OpenCode
- **Tipo**: Agente de c√≥digo AI open-source
- **Interface**: Terminal UI (TUI) + Desktop App + Web Console
- **Arquitetura**: Cliente/Servidor (server local + m√∫ltiplos clientes)
- **Linguagem**: TypeScript/Bun
- **Foco**: Terminal-first, experi√™ncia de desenvolvedor

### Forge Agent
- **Tipo**: Agente de c√≥digo AI
- **Interface**: Web UI (React/Vite)
- **Arquitetura**: API REST (FastAPI) + Frontend
- **Linguagem**: Python (backend) + TypeScript (frontend)
- **Foco**: Acesso via web/mobile, multi-sess√£o

---

## üèóÔ∏è Arquitetura

### OpenCode

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    OpenCode Server                       ‚îÇ
‚îÇ  (Hono/Bun - roda localmente na m√°quina do dev)         ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ   Sessions   ‚îÇ  ‚îÇ   Projects   ‚îÇ  ‚îÇ    Tools     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  (Storage)   ‚îÇ  ‚îÇ  (Git/VCS)   ‚îÇ  ‚îÇ  (Registry)  ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ     LSP      ‚îÇ  ‚îÇ     PTY      ‚îÇ  ‚îÇ   Snapshot  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  (Language   ‚îÇ  ‚îÇ  (Terminal   ‚îÇ  ‚îÇ  (Git diff) ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ   Server)    ‚îÇ  ‚îÇ   Sessions)  ‚îÇ  ‚îÇ             ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ                    ‚îÇ                    ‚îÇ
         ‚ñº                    ‚ñº                    ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   TUI Client ‚îÇ  ‚îÇ Desktop App  ‚îÇ  ‚îÇ Web Console ‚îÇ
‚îÇ  (Terminal)  ‚îÇ  ‚îÇ   (Tauri)    ‚îÇ  ‚îÇ  (SolidJS)  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Caracter√≠sticas principais:**
- **Server local**: Roda na m√°quina do desenvolvedor
- **M√∫ltiplos clientes**: TUI, Desktop, Web podem se conectar ao mesmo server
- **PTY (Pseudo-Terminal)**: Cada sess√£o tem um PTY persistente para comandos shell
- **Storage baseado em arquivos**: JSON files em `~/.opencode/storage/`
- **Projeto = Git repo**: Identifica projetos pelo commit root do Git

### Forge Agent

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              FastAPI Backend (Python)                   ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ   Sessions   ‚îÇ  ‚îÇ   Planner   ‚îÇ  ‚îÇ  Executor   ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  (SQLite)    ‚îÇ  ‚îÇ   (LLM)     ‚îÇ  ‚îÇ  (Tools)    ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ    Tools     ‚îÇ  ‚îÇ   Storage    ‚îÇ  ‚îÇ   Tmux      ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  (Registry)   ‚îÇ  ‚îÇ  (SQLite)    ‚îÇ  ‚îÇ  (Sessions) ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
                         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              React Frontend (Vite)                       ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  ChatPage    ‚îÇ  ‚îÇ  Components  ‚îÇ  ‚îÇ   API Client ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  (Sessions)  ‚îÇ  ‚îÇ  (Viewers)   ‚îÇ  ‚îÇ   (HTTP)     ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Caracter√≠sticas principais:**
- **API REST**: Backend separado do frontend
- **SQLite**: Banco de dados para sess√µes e mensagens
- **Tmux**: Sess√µes persistentes para contexto de shell
- **Web-first**: Acesso via navegador/mobile

---

## üîë Diferen√ßas Principais

### 1. **Execu√ß√£o de Comandos**

#### OpenCode
- **PTY (Pseudo-Terminal)**: Usa `bun-pty` para criar terminais virtuais
- **Sess√µes PTY persistentes**: Cada sess√£o pode ter m√∫ltiplos PTYs
- **WebSocket**: Clientes se conectam via WebSocket para interagir com PTY
- **Comandos executam diretamente**: `spawn()` com shell nativo
- **Contexto mantido**: Cada PTY mant√©m seu pr√≥prio estado (cwd, env vars)

```typescript
// opencode/packages/opencode/src/pty/index.ts
export async function create(input: CreateInput) {
  const ptyProcess = spawn(command, args, {
    name: "xterm-256color",
    cwd,
    env,
  })
  // PTY persiste e mant√©m estado
}
```

#### Forge Agent
- **Tmux**: Usa sess√µes tmux para manter contexto
- **send_keys**: Executa comandos via `tmux send-keys`
- **Captura output**: Usa `capture-pane` para pegar output
- **Uma sess√£o tmux por sess√£o do agente**: Mapeamento 1:1

```python
# agent/tools/tmux.py
async def execute_command(self, session_name: str, command: str):
    pane.send_keys(command, enter=True)
    # Captura output do pane
    result = pane.cmd('capture-pane', '-p')
```

**Vantagem OpenCode**: PTY √© mais leve e nativo, n√£o precisa de tmux instalado
**Vantagem Forge Agent**: Tmux √© mais comum em servidores Linux, permite reattach manual

---

### 2. **Gerenciamento de Mem√≥ria/Contexto**

#### OpenCode
- **Sem vector DB**: N√£o usa embeddings ou vector database
- **Compaction**: Usa LLM para compactar conversas antigas em resumos
- **Summary**: Gera resumos de sess√µes e mensagens
- **Pruning**: Remove outputs de tool calls antigos quando necess√°rio
- **Storage em arquivos**: JSON files organizados por projeto/sess√£o

```typescript
// opencode/packages/opencode/src/session/compaction.ts
export async function process(input: {
  messages: MessageV2.WithParts[]
  sessionID: string
}) {
  // Usa LLM para criar um resumo da conversa
  // Remove mensagens antigas, mant√©m apenas o resumo
}
```

**Estrat√©gia de Compaction:**
1. Quando tokens excedem limite do modelo
2. Cria uma mensagem de "compaction" usando LLM
3. Resumo cont√©m: o que foi feito, arquivos trabalhados, pr√≥ximos passos
4. Remove mensagens antigas, mant√©m apenas o resumo

#### Forge Agent
- **Sem vector DB**: Tamb√©m n√£o usa (ainda)
- **SQLite**: Armazena todas as mensagens
- **Sem compaction**: Mant√©m todas as mensagens
- **Context limitado**: Usa √∫ltimas N mensagens no prompt

**Problema atual**: Pode exceder limites de contexto do modelo com sess√µes longas

---

### 3. **Estrutura de Dados**

#### OpenCode

```
Storage Structure:
storage/
  ‚îú‚îÄ‚îÄ session/
  ‚îÇ   ‚îî‚îÄ‚îÄ {projectID}/
  ‚îÇ       ‚îî‚îÄ‚îÄ {sessionID}.json    # Session metadata
  ‚îú‚îÄ‚îÄ message/
  ‚îÇ   ‚îî‚îÄ‚îÄ {sessionID}/
  ‚îÇ       ‚îî‚îÄ‚îÄ {messageID}.json    # Message metadata
  ‚îú‚îÄ‚îÄ part/
  ‚îÇ   ‚îî‚îÄ‚îÄ {messageID}/
  ‚îÇ       ‚îî‚îÄ‚îÄ {partID}.json       # Message parts (text, tool calls, etc.)
  ‚îî‚îÄ‚îÄ session_diff/
      ‚îî‚îÄ‚îÄ {sessionID}.json        # Git diffs da sess√£o
```

**Hierarquia:**
- **Project** ‚Üí identificado pelo commit root do Git
- **Session** ‚Üí pertence a um projeto, pode ter parentID (child sessions)
- **Message** ‚Üí pertence a uma sess√£o, tem role (user/assistant)
- **Part** ‚Üí partes de uma mensagem (text, tool-invocation, file, etc.)

#### Forge Agent

```
Database Structure:
sessions/
  ‚îú‚îÄ‚îÄ session_id (PK)
  ‚îú‚îÄ‚îÄ title
  ‚îú‚îÄ‚îÄ created_at
  ‚îú‚îÄ‚îÄ updated_at
  ‚îî‚îÄ‚îÄ tmux_session        # Nome da sess√£o tmux

messages/
  ‚îú‚îÄ‚îÄ message_id (PK)
  ‚îú‚îÄ‚îÄ session_id (FK)
  ‚îú‚îÄ‚îÄ role
  ‚îú‚îÄ‚îÄ content
  ‚îú‚îÄ‚îÄ plan_result (JSON)
  ‚îî‚îÄ‚îÄ execution_result (JSON)
```

**Hierarquia:**
- **Session** ‚Üí sess√£o de chat
- **Message** ‚Üí mensagem na sess√£o
- **Plan/Execution** ‚Üí armazenados como JSON na mensagem

---

### 4. **Tools e Execu√ß√£o**

#### OpenCode

**Tools dispon√≠veis:**
- `bash` - Executa comandos shell (com parsing de tree-sitter)
- `read` - L√™ arquivos
- `write` - Escreve arquivos
- `edit` - Edita arquivos (multiedit)
- `grep` - Busca em arquivos
- `ls` - Lista diret√≥rios
- `patch` - Aplica patches
- `lsp` - Integra√ß√£o com Language Server Protocol
- `codesearch` - Busca sem√¢ntica (usando LSP)
- `skill` - Skills customizadas
- `task` - Gerenciamento de tarefas
- `websearch` - Busca web
- `webfetch` - Fetch de URLs

**Caracter√≠sticas:**
- **Tree-sitter parsing**: Parse de comandos bash para valida√ß√£o
- **Permission system**: Sistema de permiss√µes granular
- **Tool registry**: Registry centralizado de tools
- **Tool metadata**: Tools podem retornar metadata al√©m de output

#### Forge Agent

**Tools dispon√≠veis:**
- `shell` - Executa comandos shell
- `filesystem` - Opera√ß√µes de arquivo (read, write, list, create, delete)
- `git` - Opera√ß√µes Git
- `github` - API GitHub + GitHub CLI
- `system` - Informa√ß√µes do sistema
- `tmux` - Gerenciamento de sess√µes tmux

**Caracter√≠sticas:**
- **Path validation**: Valida√ß√£o obrigat√≥ria de paths
- **Approval system**: Sistema de aprova√ß√£o para opera√ß√µes destrutivas
- **Tmux integration**: Tudo executa no tmux quando session_id presente

---

## üéØ Como OpenCode Resolve Problemas que Tivemos

### 1. **Persist√™ncia de Diret√≥rio (cd n√£o funciona)**

**Problema**: Comandos `cd` n√£o persistem entre execu√ß√µes

**OpenCode:**
- Usa `workdir` parameter no tool bash
- N√£o executa `cd` - sempre passa `cwd` para `spawn()`
- Cada comando pode especificar seu pr√≥prio `workdir`

```typescript
// opencode/packages/opencode/src/tool/bash.ts
const proc = spawn(params.command, {
  shell,
  cwd: params.workdir || Instance.directory,  // Sempre especifica cwd
  // ...
})
```

**Forge Agent (nossa solu√ß√£o):**
- Usa tmux para manter contexto
- `cd` executa no tmux via `send_keys`
- Pr√≥ximos comandos herdam o diret√≥rio do tmux

**Compara√ß√£o:**
- **OpenCode**: Mais expl√≠cito, cada comando especifica onde executar
- **Forge Agent**: Mais impl√≠cito, contexto persiste automaticamente

---

### 2. **Gerenciamento de Contexto/Mem√≥ria**

**Problema**: Sess√µes longas excedem limites de contexto do modelo

**OpenCode:**
- **Compaction autom√°tica**: Quando tokens excedem limite, compacta automaticamente
- **Summary**: Gera resumos de sess√µes e mensagens
- **Pruning**: Remove outputs de tool calls antigos

```typescript
// opencode/packages/opencode/src/session/compaction.ts
export async function isOverflow(input: {
  tokens: MessageV2.Assistant["tokens"]
  model: Provider.Model
}) {
  const context = input.model.limit.context
  const count = input.tokens.input + input.tokens.cache.read + input.tokens.output
  const usable = input.model.limit.input || context - output
  return count > usable  // Detecta overflow
}
```

**Forge Agent:**
- **Sem compaction**: Mant√©m todas as mensagens
- **Context limitado**: Usa apenas √∫ltimas N mensagens
- **Problema**: Pode perder contexto importante em sess√µes longas

**Solu√ß√£o recomendada para Forge Agent:**
- Implementar compaction similar ao OpenCode
- Usar LLM para gerar resumos de conversas antigas
- Manter apenas resumos + mensagens recentes

---

### 3. **Execu√ß√£o de Comandos**

**Problema**: Comandos precisam manter contexto (cwd, env vars)

**OpenCode:**
- **PTY persistente**: Cada sess√£o pode ter m√∫ltiplos PTYs
- **WebSocket streaming**: Output √© streamed em tempo real
- **Buffer management**: Mant√©m buffer de output (2MB limit)

**Forge Agent:**
- **Tmux session**: Uma sess√£o tmux por sess√£o do agente
- **send_keys**: Executa comandos via tmux
- **capture-pane**: Captura output ap√≥s execu√ß√£o

**Compara√ß√£o:**
- **OpenCode PTY**: Mais leve, nativo, melhor para streaming
- **Forge Agent Tmux**: Mais comum em servidores, permite reattach manual

---

### 4. **Estrutura de Mensagens**

**OpenCode:**
- **MessageV2**: Estrutura rica com parts
- **Parts**: text, tool-invocation, file, reasoning, snapshot, patch
- **Hierarquia**: Message ‚Üí Parts (m√∫ltiplos tipos)
- **Metadata**: Cada part pode ter metadata rica

```typescript
// opencode/packages/opencode/src/session/message-v2.ts
export const MessageV2 = {
  TextPart: { type: "text", text: string },
  ToolInvocationPart: { type: "tool-invocation", toolInvocation: {...} },
  FilePart: { type: "file", url: string, source: {...} },
  ReasoningPart: { type: "reasoning", text: string },
  SnapshotPart: { type: "snapshot", snapshot: string },
  PatchPart: { type: "patch", files: string[], hash: string },
}
```

**Forge Agent:**
- **Message simples**: role + content
- **Plan/Execution**: Armazenados como JSON na mensagem
- **Estrutura mais simples**: Menos flex√≠vel, mas mais direta

---

## üí° Li√ß√µes Aprendidas

### 1. **Compaction √© Essencial**
OpenCode mostra que compaction autom√°tica √© crucial para sess√µes longas. Sem isso, o contexto explode.

**Recomenda√ß√£o para Forge Agent:**
- Implementar compaction quando tokens excedem limite
- Usar LLM para gerar resumos
- Manter apenas resumos + mensagens recentes

### 2. **PTY vs Tmux**
Ambos funcionam, mas t√™m trade-offs:
- **PTY**: Mais leve, melhor para streaming, n√£o precisa de depend√™ncia externa
- **Tmux**: Mais comum, permite reattach manual, j√° temos implementado

**Recomenda√ß√£o**: Manter tmux, mas considerar PTY no futuro se precisarmos de streaming melhor

### 3. **Estrutura de Mensagens Rica**
OpenCode usa uma estrutura muito mais rica para mensagens (parts), o que permite:
- Melhor organiza√ß√£o
- Metadata rica
- Suporte a m√∫ltiplos tipos de conte√∫do

**Recomenda√ß√£o**: Considerar evoluir estrutura de mensagens para suportar parts

### 4. **Storage em Arquivos vs Database**
- **OpenCode (arquivos)**: Mais simples, f√°cil de debugar, version√°vel
- **Forge Agent (SQLite)**: Mais estruturado, queries mais f√°ceis

**Ambos funcionam**, mas arquivos podem ser mais simples para desenvolvimento

### 5. **Client/Server Architecture**
OpenCode usa client/server, permitindo:
- M√∫ltiplos clientes (TUI, Desktop, Web)
- Server roda localmente
- Clientes se conectam via WebSocket/HTTP

**Forge Agent** j√° tem isso (API REST), mas poderia adicionar WebSocket para streaming

---

## üîÑ Vector DB vs Sessions

### OpenCode N√ÉO usa Vector DB
- Usa **compaction** (resumos via LLM)
- Usa **summary** (resumos de sess√µes)
- Usa **pruning** (remove outputs antigos)

### Por que n√£o Vector DB?
1. **Custo**: Embeddings s√£o caros
2. **Lat√™ncia**: Adiciona lat√™ncia √†s queries
3. **Complexidade**: Adiciona infraestrutura
4. **Compaction funciona**: LLM consegue resumir bem o contexto

### Quando Vector DB faz sentido?
- **Codebase muito grande**: Quando precisa buscar em milh√µes de arquivos
- **Busca sem√¢ntica**: Quando precisa encontrar c√≥digo similar
- **RAG**: Quando precisa recuperar contexto relevante de c√≥digo

**OpenCode usa LSP para busca sem√¢ntica**, n√£o vector DB.

---

## üñ•Ô∏è Gerenciamento de Web + Terminal (PTY)

### Como OpenCode Resolve o Problema

**A chave**: OpenCode **separa completamente** Agent Sessions de PTY Sessions!

#### 1. **Agent Sessions (Session)**
- **Prop√≥sito**: Conversas com LLM, execu√ß√£o de tools
- **Storage**: Arquivos JSON em `storage/session/{projectID}/{sessionID}.json`
- **Lifetime**: Persistem at√© serem deletadas
- **Tools**: Executam comandos via `spawn()` diretamente, **N√ÉO usam PTY**

```typescript
// opencode/packages/opencode/src/tool/bash.ts
// Tool bash executa comandos diretamente, n√£o via PTY
const proc = spawn(params.command, {
  shell,
  cwd: params.workdir || Instance.directory,  // Sempre especifica cwd
  stdio: ["ignore", "pipe", "pipe"],
})
// Captura stdout/stderr diretamente
```

#### 2. **PTY Sessions (Pty)**
- **Prop√≥sito**: Terminais interativos para UI web/desktop
- **Storage**: **Em mem√≥ria apenas** (n√£o persistem)
- **Lifetime**: Existem apenas enquanto ativas
- **Conex√£o**: WebSocket para streaming em tempo real
- **Uso**: Apenas para mostrar terminal na UI, **n√£o usado por tools**

```typescript
// opencode/packages/opencode/src/pty/index.ts
interface ActiveSession {
  info: Info
  process: IPty
  buffer: string
  subscribers: Set<WSContext>  // M√∫ltiplos clientes podem se conectar
}

// WebSocket connection
export function connect(id: string, ws: WSContext) {
  session.subscribers.add(ws)  // Adiciona cliente
  // Envia buffer existente
  // Retorna handlers para onMessage/onClose
}
```

#### 3. **Arquitetura de Separa√ß√£o**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              OpenCode Server                             ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îÇ
‚îÇ  ‚îÇ Agent Sessions   ‚îÇ      ‚îÇ  PTY Sessions    ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ                  ‚îÇ      ‚îÇ                  ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ - Conversas LLM  ‚îÇ      ‚îÇ - Terminais      ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ - Tools exec     ‚îÇ      ‚îÇ - WebSocket      ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ - Storage JSON   ‚îÇ      ‚îÇ - Em mem√≥ria      ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ - Persistem      ‚îÇ      ‚îÇ - N√£o persistem  ‚îÇ       ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îÇ
‚îÇ         ‚îÇ                          ‚îÇ                    ‚îÇ
‚îÇ         ‚îÇ                          ‚îÇ                    ‚îÇ
‚îÇ         ‚ñº                          ‚ñº                    ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îÇ
‚îÇ  ‚îÇ  Tool: bash      ‚îÇ      ‚îÇ  WebSocket API   ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ  spawn() direto  ‚îÇ      ‚îÇ  /pty/:id/connect‚îÇ       ‚îÇ
‚îÇ  ‚îÇ  N√ÉO usa PTY     ‚îÇ      ‚îÇ                  ‚îÇ       ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ                          ‚îÇ
         ‚ñº                          ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Web UI          ‚îÇ      ‚îÇ  Web Terminal UI ‚îÇ
‚îÇ  (Chat)          ‚îÇ      ‚îÇ  (Terminal)      ‚îÇ
‚îÇ                  ‚îÇ      ‚îÇ                  ‚îÇ
‚îÇ  - HTTP REST     ‚îÇ      ‚îÇ  - WebSocket     ‚îÇ
‚îÇ  - Sessions      ‚îÇ      ‚îÇ  - PTY streaming ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Por que essa Separa√ß√£o Funciona?

1. **Tools n√£o precisam de terminal interativo**
   - Tools executam comandos via `spawn()`, capturam output, retornam resultado
   - N√£o precisam de terminal "vivo" com prompt, hist√≥rico, etc.
   - Cada comando √© independente e especifica seu pr√≥prio `cwd`

2. **PTY √© apenas para UX (experi√™ncia do usu√°rio)**
   - **Terminal interativo na web**: Usu√°rio pode digitar comandos diretamente, ver output em tempo real
   - **Debug manual**: Usu√°rio pode testar comandos antes de pedir ao agente
   - **M√∫ltiplas abas**: Usu√°rio pode ter v√°rios terminais abertos simultaneamente
   - **Terminal √© opcional**: N√£o √© necess√°rio para o agente funcionar, √© apenas uma conveni√™ncia
   - **M√∫ltiplos clientes**: V√°rios clientes podem se conectar ao mesmo PTY (colabora√ß√£o)

3. **Sem conflito de estado**
   - Agent sessions n√£o interferem com PTY sessions
   - Cada um tem seu pr√≥prio ciclo de vida
   - PTY pode ser criado/destru√≠do independentemente

### Para que serve o PTY ent√£o?

**PTY = Terminal Interativo na Web (como um terminal "normal" no navegador)**

```typescript
// opencode/packages/app/src/components/terminal.tsx
// Usu√°rio pode:
// 1. Digitar comandos diretamente no terminal
// 2. Ver output em tempo real via WebSocket
// 3. Ter m√∫ltiplas abas de terminal
// 4. Usar como um terminal "normal" para debug manual

t.onData((data) => {
  // Envia input do usu√°rio para o PTY
  socket.send(data)
})

socket.addEventListener("message", (event) => {
  // Mostra output do PTY no terminal
  t.write(event.data)
})
```

### Estrutura da Interface OpenCode

**A interface tem 3 formas de intera√ß√£o separadas:**

1. **PromptInput (modo normal)**: Chat com o agente
   - Usu√°rio digita mensagens em linguagem natural
   - Agente responde e executa tools automaticamente
   - Tools executam via `spawn()` direto (n√£o PTY)

2. **PromptInput (modo shell)**: Executa comando shell via agente
   - Usu√°rio digita comando (ex: `ls -la`)
   - Chama `session.shell()` que executa via `spawn()` direto
   - **N√ÉO usa PTY** - apenas executa e mostra resultado no chat

3. **Terminal PTY**: Terminal interativo separado
   - Painel opcional na parte inferior da tela
   - Usu√°rio digita comandos diretamente no terminal
   - **Completamente separado** do agente e do chat
   - Apenas para conveni√™ncia do usu√°rio (debug manual, explora√ß√£o)

**Caso de uso t√≠pico:**
```
1. Usu√°rio abre terminal PTY na web (painel inferior)
2. Digita `ls -la` manualmente no terminal para ver arquivos
3. Depois usa PromptInput (chat) e pede: "crie um arquivo novo.txt"
4. Agente executa via tool `bash` (spawn direto, n√£o via PTY)
5. Usu√°rio pode verificar no terminal PTY manualmente depois
```

**Resumo:**
- **Chat com agente (PromptInput)**: Intera√ß√£o via linguagem natural
- **Modo shell (PromptInput)**: Executa comando via `spawn()` (n√£o PTY)
- **Terminal PTY**: Terminal interativo separado, apenas para o usu√°rio
- **Tools do agente**: Executam via `spawn()` direto, n√£o usam PTY
- **PTY**: Apenas para terminal interativo na UI, opcional, para conveni√™ncia do usu√°rio

### Compara√ß√£o com Forge Agent

**Forge Agent (nossa abordagem atual):**
- **Uma sess√£o tmux por sess√£o do agente**: Mapeamento 1:1
- **Tools executam no tmux**: Todos comandos via `send_keys`
- **Problema**: Mistura execu√ß√£o de tools com terminal interativo
- **Vantagem**: Contexto persiste automaticamente (cwd, env vars)

**OpenCode (abordagem deles):**
- **Agent sessions independentes de PTY**: Separa√ß√£o completa
- **Tools executam diretamente**: `spawn()` com `cwd` expl√≠cito
- **PTY opcional**: Apenas para UI, n√£o usado por tools
- **Vantagem**: Mais simples, sem depend√™ncia de terminal

### Como Resolver no Forge Agent?

**Op√ß√£o 1: Manter Tmux (atual)**
- ‚úÖ Contexto persiste automaticamente
- ‚úÖ Funciona bem para nosso caso
- ‚ùå Mistura execu√ß√£o com terminal interativo
- ‚ùå Depend√™ncia de tmux

**Op√ß√£o 2: Separar como OpenCode**
- ‚úÖ Separa√ß√£o clara de responsabilidades
- ‚úÖ Tools mais simples (spawn direto)
- ‚úÖ Terminal opcional para UI
- ‚ùå Precisa passar `cwd` explicitamente em cada comando
- ‚ùå Perde persist√™ncia autom√°tica de contexto

**Op√ß√£o 3: H√≠brido (Recomendado)**
- **Para tools**: Usar `spawn()` direto com `cwd` do tmux (n√£o via send_keys)
- **Para terminal UI**: Criar PTY separado opcional
- **Manter tmux**: Apenas para obter `cwd` atual, n√£o para execu√ß√£o

```python
# Abordagem h√≠brida
async def execute_command(self, session_name: str, command: str):
    # 1. Obter cwd do tmux
    cwd = await self.get_working_directory(session_name)
    
    # 2. Executar diretamente com spawn (n√£o via send_keys)
    proc = await asyncio.create_subprocess_exec(
        command,
        cwd=cwd,  # Usa cwd do tmux
        stdout=asyncio.subprocess.PIPE,
        stderr=asyncio.subprocess.PIPE,
    )
    
    # 3. Capturar output
    stdout, stderr = await proc.communicate()
    
    # 4. Se comando foi 'cd', atualizar tmux tamb√©m
    if command.startswith('cd '):
        await self.update_tmux_cwd(session_name, new_cwd)
```

**Vantagens da abordagem h√≠brida:**
- ‚úÖ Execu√ß√£o mais confi√°vel (n√£o depende de capture-pane)
- ‚úÖ Output capturado corretamente
- ‚úÖ Mant√©m contexto do tmux para cwd
- ‚úÖ Permite adicionar terminal UI opcional depois

---

## üîß Arquitetura de Tools e Permiss√µes

### Como OpenCode Estrutura suas Tools

**OpenCode usa um sistema de Registry de Tools similar ao Forge Agent, mas com algumas diferen√ßas importantes:**

#### 1. **Estrutura de Tools**

```typescript
// opencode/packages/opencode/src/tool/registry.ts
// Tools s√£o registradas em um registry central
export namespace ToolRegistry {
  async function all(): Promise<Tool.Info[]> {
    return [
      InvalidTool,
      QuestionTool,      // Apenas para clientes (app/cli/desktop)
      BashTool,         // Executa comandos shell
      ReadTool,         // L√™ arquivos
      GlobTool,         // Busca arquivos por padr√£o
      GrepTool,         // Busca texto em arquivos
      EditTool,         // Edita arquivos (find/replace)
      WriteTool,        // Escreve arquivos
      TaskTool,          // Gerencia tarefas
      WebFetchTool,     // Busca na web
      TodoWriteTool,    // Escreve TODOs
      TodoReadTool,     // L√™ TODOs
      WebSearchTool,     // Busca web (exa)
      CodeSearchTool,    // Busca c√≥digo (exa)
      SkillTool,        // Skills customizados
      LspTool,          // Language Server Protocol
      BatchTool,        // Executa m√∫ltiplas tools
      PlanEnterTool,    // Entra em modo plan
      PlanExitTool,     // Sai de modo plan
      ...custom,        // Tools customizadas de plugins
    ]
  }
}
```

**Compara√ß√£o com Forge Agent:**
- **Forge Agent**: Tools espec√≠ficas (filesystem, git, github, shell, system, tmux)
- **OpenCode**: Tools mais granulares (read, write, edit, grep, glob, bash, etc.)

#### 2. **Sistema de Permiss√µes**

**OpenCode usa um sistema de permiss√µes baseado em Rulesets:**

```typescript
// opencode/packages/opencode/src/permission/next.ts
export namespace PermissionNext {
  export type Action = "allow" | "deny" | "ask"
  
  export type Rule = {
    permission: string  // Ex: "bash", "edit", "read"
    pattern: string     // Ex: "*", "*.py", "/tmp/*"
    action: Action      // allow, deny, ou ask
  }
  
  export type Ruleset = Rule[]
}
```

**Como funciona:**

1. **Permiss√µes por Tool**: Cada tool tem uma permiss√£o associada
   - `bash` ‚Üí permiss√£o `"bash"`
   - `edit`, `write`, `patch`, `multiedit` ‚Üí permiss√£o `"edit"`
   - `read` ‚Üí permiss√£o `"read"`
   - `grep`, `glob` ‚Üí permiss√£o `"grep"`

2. **Patterns com Wildcards**: Permiss√µes podem ser espec√≠ficas por padr√£o
   ```typescript
   {
     permission: "bash",
     pattern: "rm -rf *",      // Comando espec√≠fico
     action: "deny"
   }
   {
     permission: "edit",
     pattern: "*.py",           // Arquivos Python
     action: "ask"
   }
   {
     permission: "read",
     pattern: "/tmp/*",         // Diret√≥rio espec√≠fico
     action: "deny"
   }
   ```

3. **A√ß√µes:**
   - **`allow`**: Permite automaticamente
   - **`deny`**: Bloqueia automaticamente
   - **`ask`**: Pede confirma√ß√£o ao usu√°rio

4. **Verifica√ß√£o durante execu√ß√£o:**
   ```typescript
   // opencode/packages/opencode/src/tool/bash.ts
   async execute(params, ctx) {
     // 1. Parse do comando para extrair patterns
     const patterns = extractPatterns(params.command)
     
     // 2. Verifica permiss√µes ANTES de executar
     await ctx.ask({
       permission: "bash",
       patterns: Array.from(patterns),
       always: Array.from(always),
       metadata: {},
     })
     
     // 3. S√≥ executa se permitido
     const proc = spawn(params.command, { ... })
   }
   ```

#### 3. **Sistema de Aprova√ß√£o (HITL)**

**OpenCode implementa Human-in-the-Loop (HITL) de forma mais sofisticada:**

```typescript
// Quando action === "ask", o sistema:
// 1. Publica evento de permiss√£o pendente
Bus.publish(Event.Asked, {
  id: permissionID,
  sessionID,
  permission: "bash",
  patterns: ["rm -rf *"],
  metadata: { command: "rm -rf /tmp/*" }
})

// 2. Usu√°rio pode responder:
// - "once": Permite apenas esta vez
// - "always": Permite sempre para este pattern
// - "reject": Rejeita e para execu√ß√£o

// 3. Se "always", salva no ruleset para futuras execu√ß√µes
```

**Compara√ß√£o com Forge Agent:**
- **Forge Agent**: Aprova√ß√£o bin√°ria (sim/n√£o) por opera√ß√£o
- **OpenCode**: Aprova√ß√£o com op√ß√µes (once/always/reject) e persist√™ncia de regras

#### 4. **Limita√ß√µes e Valida√ß√µes**

**OpenCode implementa v√°rias camadas de valida√ß√£o:**

1. **Valida√ß√£o de Schema (Zod)**:
   ```typescript
   parameters: z.object({
     filePath: z.string().describe("The path to the file"),
     offset: z.number().optional(),
     limit: z.number().optional(),
   })
   ```

2. **Valida√ß√£o de Path**:
   ```typescript
   // Verifica se path est√° dentro do projeto
   await assertExternalDirectory(ctx, filepath)
   ```

3. **Valida√ß√£o de Comando (bash tool)**:
   ```typescript
   // Parse do comando com tree-sitter
   const tree = await parser().parse(params.command)
   // Extrai patterns e diret√≥rios
   // Verifica permiss√µes antes de executar
   ```

4. **Prote√ß√£o contra Doom Loops**:
   ```typescript
   // Detecta se mesma tool foi chamada 3x com mesmos par√¢metros
   if (lastThree.every(p => 
     p.tool === toolName && 
     JSON.stringify(p.input) === JSON.stringify(currentInput)
   )) {
     await PermissionNext.ask({
       permission: "doom_loop",
       patterns: [toolName],
       ...
     })
   }
   ```

#### 5. **Tools Customizadas**

**OpenCode permite tools customizadas via plugins:**

```typescript
// Tools podem ser carregadas de:
// 1. Diret√≥rios do projeto: {tool,tools}/*.{js,ts}
// 2. Plugins instalados
// 3. MCP (Model Context Protocol) servers

const custom = []
for (const dir of await Config.directories()) {
  for await (const match of glob.scan("tool/*.{js,ts}")) {
    const mod = await import(match)
    custom.push(fromPlugin(id, mod))
  }
}
```

### Compara√ß√£o: OpenCode vs Forge Agent

| Aspecto | OpenCode | Forge Agent |
|---------|----------|-------------|
| **Estrutura** | Tools granulares (read, write, edit, grep) | Tools por dom√≠nio (filesystem, git, shell) |
| **Permiss√µes** | Ruleset com patterns e wildcards | Opera√ß√µes espec√≠ficas (APPROVAL_REQUIRED) |
| **Aprova√ß√£o** | once/always/reject com persist√™ncia | Sim/N√£o bin√°rio |
| **Valida√ß√£o** | Schema + Path + Command parsing | Schema + Path validation |
| **Customiza√ß√£o** | Plugins + MCP + arquivos locais | Apenas c√≥digo |
| **Doom Loop** | ‚úÖ Detecta loops | ‚ùå N√£o tem |
| **External Dir** | ‚úÖ Verifica diret√≥rios externos | ‚ùå N√£o tem |

### Vantagens da Abordagem OpenCode

1. **Granularidade**: Tools mais espec√≠ficas permitem controle fino
2. **Flexibilidade**: Patterns com wildcards permitem regras complexas
3. **Persist√™ncia**: Regras "always" s√£o salvas automaticamente
4. **Extensibilidade**: Plugins e MCP permitem extens√£o f√°cil
5. **Seguran√ßa**: M√∫ltiplas camadas de valida√ß√£o

### Vantagens da Abordagem Forge Agent

1. **Simplicidade**: Tools por dom√≠nio s√£o mais f√°ceis de entender
2. **Menos overhead**: Menos tools = menos overhead de registro
3. **Agrupamento l√≥gico**: Opera√ß√µes relacionadas ficam juntas
4. **Mais direto**: Aprova√ß√£o bin√°ria √© mais simples

### Recomenda√ß√µes para Forge Agent

**Curto Prazo:**
1. Adicionar detec√ß√£o de doom loops
2. Melhorar sistema de aprova√ß√£o (once/always/reject)
3. Adicionar valida√ß√£o de comandos bash (parsing)

**M√©dio Prazo:**
1. Considerar sistema de rulesets com patterns
2. Adicionar suporte a plugins customizados
3. Implementar persist√™ncia de regras de aprova√ß√£o

**Longo Prazo:**
1. Suporte a MCP (Model Context Protocol)
2. Tools mais granulares se necess√°rio
3. Sistema de wildcards para permiss√µes

---

## ü§ñ Integra√ß√£o com LLMs

### Como OpenCode se Comunica com LLMs

**OpenCode usa o Vercel AI SDK (`ai`) com suporte a m√∫ltiplos provedores:**

#### 1. **Provedores Suportados**

OpenCode suporta **75+ provedores de LLM** atrav√©s de:

1. **Provedores Bundled** (inclu√≠dos diretamente):
   - OpenAI, Anthropic, Google, Azure, Mistral, Groq
   - DeepInfra, TogetherAI, Perplexity, Vercel
   - Amazon Bedrock, Vertex AI, X.AI, Cohere
   - E mais...

2. **OpenAI-Compatible** (`@ai-sdk/openai-compatible`):
   - Qualquer servidor compat√≠vel com API OpenAI
   - **Ollama** (localhost:11434)
   - **LM Studio** (localhost:1234)
   - **LocalAI** (qualquer porta)
   - **Llama.cpp server** (qualquer porta)

3. **Models.dev** (descoberta autom√°tica):
   - Busca lista de modelos de `https://models.dev/api.json`
   - Atualiza automaticamente a cada hora
   - Permite descobrir novos modelos sem atualizar c√≥digo

#### 2. **Configura√ß√£o de LLMs Locais**

**OpenCode suporta LLMs locais e gratuitas via configura√ß√£o manual:**

```json
// opencode.json
{
  "provider": {
    "ollama": {
      "npm": "@ai-sdk/openai-compatible",
      "name": "Ollama (local)",
      "options": {
        "baseURL": "http://localhost:11434/v1"
      },
      "models": {
        "llama2": {
          "name": "Llama 2"
        },
        "mistral": {
          "name": "Mistral"
        }
      }
    },
    "lmstudio": {
      "npm": "@ai-sdk/openai-compatible",
      "name": "LM Studio (local)",
      "options": {
        "baseURL": "http://127.0.0.1:1234/v1"
      },
      "models": {
        "google/gemma-3n-e4b": {
          "name": "Gemma 3n-e4b (local)"
        }
      }
    }
  }
}
```

**Como funciona:**
1. Usa `@ai-sdk/openai-compatible` que aceita qualquer `baseURL`
2. Configura `baseURL` para apontar ao servidor local
3. Define modelos dispon√≠veis manualmente
4. N√£o precisa de API key (pode usar `"apiKey": "not-needed"`)

#### 3. **Models.dev - Descoberta Autom√°tica**

**OpenCode usa `models.dev` para descobrir providers automaticamente:**

```typescript
// opencode/packages/opencode/src/provider/models.ts
export async function get() {
  // 1. Tenta ler cache local
  const file = Bun.file(filepath)
  const result = await file.json().catch(() => {})
  if (result) return result
  
  // 2. Busca de models.dev
  const json = await fetch("https://models.dev/api.json").then((x) => x.text())
  return JSON.parse(json)
}

// Atualiza automaticamente a cada hora
setInterval(() => ModelsDev.refresh(), 60 * 1000 * 60)
```

**Vantagens:**
- Descobre novos modelos automaticamente
- N√£o precisa atualizar c√≥digo para novos providers
- Cache local para funcionar offline
- Metadados completos (limites, custos, capacidades)

#### 4. **Sistema de Provedores**

**OpenCode tem um sistema flex√≠vel de provedores:**

```typescript
// 1. Provedores podem vir de:
// - models.dev (descoberta autom√°tica)
// - Config manual (opencode.json)
// - Plugins customizados

// 2. Cada provedor pode ter:
{
  id: "ollama",
  name: "Ollama (local)",
  npm: "@ai-sdk/openai-compatible",  // Package a usar
  options: {
    baseURL: "http://localhost:11434/v1",
    apiKey: "not-needed",  // Opcional
    timeout: 300000,       // Opcional
  },
  models: {
    "llama2": {
      name: "Llama 2",
      limit: { context: 4096, output: 2048 },
      capabilities: { tool_call: true, temperature: true }
    }
  }
}

// 3. SDK √© criado dinamicamente:
const sdk = createOpenAICompatible({
  baseURL: "http://localhost:11434/v1"
})
const model = sdk.languageModel("llama2")
```

### Compara√ß√£o: OpenCode vs Forge Agent

| Aspecto | OpenCode | Forge Agent |
|---------|----------|-------------|
| **Provedores** | 75+ via models.dev + config | Ollama, LocalAI, AirLLM (manual) |
| **LLMs Locais** | ‚úÖ Via `openai-compatible` + config | ‚úÖ Suporte nativo (Ollama, LocalAI) |
| **Descoberta** | ‚úÖ Autom√°tica (models.dev) | ‚ùå Manual (config) |
| **Configura√ß√£o** | JSON (opencode.json) | YAML (agent.yaml) |
| **SDK** | Vercel AI SDK (`ai`) | Implementa√ß√£o pr√≥pria |
| **Gratuitas** | ‚úÖ Suporta (Ollama, LM Studio) | ‚úÖ Suporta (Ollama, LocalAI) |

### Vantagens da Abordagem OpenCode

1. **Flexibilidade**: Qualquer servidor OpenAI-compatible funciona
2. **Descoberta Autom√°tica**: Models.dev atualiza automaticamente
3. **Padr√£o**: Usa padr√£o da ind√∫stria (Vercel AI SDK)
4. **Extensibilidade**: F√°cil adicionar novos providers via config

### Vantagens da Abordagem Forge Agent

1. **Simplicidade**: Configura√ß√£o mais direta (YAML)
2. **Foco Local**: Otimizado para LLMs locais
3. **Controle**: Implementa√ß√£o pr√≥pria = mais controle
4. **Menos Depend√™ncias**: N√£o depende de models.dev

### Como Forge Agent Pode Melhorar

**Curto Prazo:**
1. Adicionar suporte a `openai-compatible` gen√©rico
2. Permitir configurar `baseURL` customizado
3. Suportar m√∫ltiplos provedores via config

**M√©dio Prazo:**
1. Considerar usar Vercel AI SDK (padr√£o da ind√∫stria)
2. Adicionar descoberta autom√°tica de modelos (opcional)
3. Suportar models.dev ou similar

**Longo Prazo:**
1. Sistema de plugins para providers customizados
2. Cache de metadados de modelos
3. UI para configurar providers facilmente

---

## üöÄ Fluxo de Execu√ß√£o e Auto-Corre√ß√£o

### Por que OpenCode Parece Mais "Fluido"?

**A diferen√ßa principal est√° no sistema de execu√ß√£o autom√°tica e loop cont√≠nuo:**

#### 1. **Execu√ß√£o Autom√°tica Durante Streaming**

**OpenCode usa Vercel AI SDK que executa tools automaticamente:**

```typescript
// opencode/packages/opencode/src/session/llm.ts
return streamText({
  tools,  // Tools s√£o passadas para o SDK
  // SDK automaticamente executa tools quando LLM chama
  // N√£o para para aprova√ß√£o a cada tool
})

// Durante o streaming:
// 1. LLM chama tool ‚Üí SDK executa imediatamente
// 2. Resultado volta para LLM ‚Üí LLM continua pensando
// 3. LLM pode auto-corrigir baseado no resultado
// 4. Loop continua at√© LLM terminar naturalmente
```

**Compara√ß√£o com Forge Agent:**
- **Forge Agent**: Para a cada step, espera aprova√ß√£o, executa, para novamente
- **OpenCode**: Executa automaticamente, LLM v√™ resultado, continua pensando

#### 2. **Loop Cont√≠nuo (N√£o Para a Cada Step)**

**OpenCode tem um loop que continua at√© a LLM terminar:**

```typescript
// opencode/packages/opencode/src/session/prompt.ts
export const loop = async (sessionID) => {
  while (true) {  // Loop continua at√© LLM terminar
    // 1. LLM gera resposta (pode incluir tool calls)
    const result = await processor.process({ ... })
    
    // 2. Se LLM chamou tools, executa automaticamente
    // 3. Resultados voltam para LLM
    // 4. LLM pode continuar pensando/corrigindo
    // 5. Loop continua at√© LLM terminar naturalmente
    
    if (result === "stop") break  // S√≥ para se erro ou usu√°rio rejeitar
    continue  // Continua o loop
  }
}
```

**Caracter√≠sticas:**
- **M√∫ltiplos steps**: LLM pode fazer v√°rios steps sem parar
- **Auto-corre√ß√£o**: LLM v√™ resultados e pode corrigir imediatamente
- **Paralelismo**: LLM pode chamar m√∫ltiplas tools em paralelo
- **Continuidade**: N√£o para entre steps

#### 3. **Aprova√ß√µes Ass√≠ncronas (N√£o Bloqueiam)**

**OpenCode usa aprova√ß√µes ass√≠ncronas que n√£o bloqueiam o loop:**

```typescript
// opencode/packages/opencode/src/session/processor.ts
case "tool-call": {
  // Tool √© chamada ‚Üí Executa automaticamente
  // Se precisa aprova√ß√£o, pede assincronamente
  await ctx.ask({
    permission: "bash",
    patterns: ["*"],
  })
  // Se usu√°rio rejeitar, para o loop
  // Se permitir, continua automaticamente
}
```

**Como funciona:**
1. Tool √© executada automaticamente
2. Se precisa aprova√ß√£o, pede em background
3. Se usu√°rio rejeitar ‚Üí loop para
4. Se permitir ‚Üí continua automaticamente
5. **N√£o bloqueia cada tool individualmente**

#### 4. **Instru√ß√µes para Auto-Corre√ß√£o**

**OpenCode instrui a LLM a auto-corrigir:**

```txt
// opencode/packages/opencode/src/session/prompt/anthropic.txt
- You can call multiple tools in a single response
- Maximize use of parallel tool calls where possible
- If a tool call fails, try a different approach
- Keep going until the problem is solved
```

```txt
// opencode/packages/opencode/src/session/prompt/beast.txt
You MUST iterate and keep going until the problem is solved.
You have everything you need to resolve this problem autonomously.
Only terminate when you are sure the problem is solved.
```

#### 5. **Sistema de Steps (N√£o Limita Rigidamente)**

**OpenCode tem limite de steps, mas √© flex√≠vel:**

```typescript
const maxSteps = agent.steps ?? Infinity  // Padr√£o: infinito
const isLastStep = step >= maxSteps

// Se √∫ltimo step, desabilita tools mas permite texto
if (isLastStep) {
  messages.push({
    role: "assistant",
    content: MAX_STEPS  // "Tools disabled, respond with text only"
  })
}
```

**Vantagens:**
- LLM pode fazer muitos steps antes de parar
- No √∫ltimo step, ainda pode responder em texto
- N√£o corta abruptamente

### Compara√ß√£o: OpenCode vs Forge Agent

| Aspecto | OpenCode | Forge Agent |
|---------|----------|-------------|
| **Execu√ß√£o** | Autom√°tica durante streaming | Manual, para a cada step |
| **Loop** | Cont√≠nuo at√© LLM terminar | Para ap√≥s cada step |
| **Auto-corre√ß√£o** | ‚úÖ LLM v√™ resultado e corrige | ‚ùå Precisa aprovar cada step |
| **Paralelismo** | ‚úÖ M√∫ltiplas tools em paralelo | ‚ùå Sequencial |
| **Aprova√ß√£o** | Ass√≠ncrona (n√£o bloqueia) | S√≠ncrona (bloqueia) |
| **Continuidade** | ‚úÖ Continua at√© resolver | ‚ùå Para ap√≥s cada a√ß√£o |
| **Fluidez** | ‚úÖ Muito fluido | ‚ùå Travado |

### Por que Forge Agent Parece Travado?

**Problemas na abordagem atual:**

1. **Para a cada step**: Precisa aprovar cada a√ß√£o individualmente
2. **Sem auto-corre√ß√£o**: LLM n√£o v√™ resultado e n√£o pode corrigir
3. **Sem paralelismo**: Executa uma tool por vez
4. **Sem continuidade**: Para ap√≥s cada step, n√£o continua automaticamente
5. **Aprova√ß√£o bloqueante**: Espera aprova√ß√£o antes de continuar

### Como Melhorar o Forge Agent

**Curto Prazo:**
1. **Executar tools automaticamente**: N√£o parar para aprova√ß√£o a cada tool
2. **Loop cont√≠nuo**: Permitir m√∫ltiplos steps sem parar
3. **Aprova√ß√£o ass√≠ncrona**: N√£o bloquear execu√ß√£o
4. **Paralelismo**: Executar m√∫ltiplas tools em paralelo

**M√©dio Prazo:**
1. **Usar Vercel AI SDK**: Para execu√ß√£o autom√°tica de tools
2. **Sistema de steps flex√≠vel**: Limite alto, n√£o cortar abruptamente
3. **Instru√ß√µes para auto-corre√ß√£o**: Instruir LLM a continuar at√© resolver

**Longo Prazo:**
1. **Streaming de resultados**: Mostrar resultados em tempo real
2. **Auto-corre√ß√£o inteligente**: Detectar erros e auto-corrigir
3. **Planejamento adaptativo**: Ajustar plano baseado em resultados

### Exemplo de Fluxo OpenCode

```
1. Usu√°rio: "Crie um arquivo novo.txt"
2. LLM: [Chama WriteTool] ‚Üí Executa automaticamente
3. Resultado: "Arquivo criado" ‚Üí Volta para LLM
4. LLM: [V√™ resultado, continua] ‚Üí "Arquivo criado com sucesso"
5. LLM: [Termina naturalmente] ‚Üí Loop para
```

### Exemplo de Fluxo Forge Agent (Atual)

```
1. Usu√°rio: "Crie um arquivo novo.txt"
2. LLM: [Gera plano com step] ‚Üí Para e espera
3. Usu√°rio: [Aprova] ‚Üí Executa
4. Resultado: "Arquivo criado" ‚Üí Para novamente
5. LLM: [N√£o v√™ resultado ainda] ‚Üí Precisa novo step
6. Usu√°rio: [Aprova novo step] ‚Üí LLM finalmente v√™ resultado
```

**Problema**: Muito mais lento, sem auto-corre√ß√£o, travado.

---

## üì¶ Vercel AI SDK - O que √© e Como Funciona

### O que √© o Vercel AI SDK?

**Vercel AI SDK √© um toolkit TypeScript/JavaScript para construir apps com LLMs:**

- **Provider-agn√≥stico**: Funciona com OpenAI, Anthropic, Google, Ollama, etc.
- **Full-stack**: Funciona no backend (Node.js/Edge) e frontend (React/Vue/Svelte)
- **Streaming nativo**: Suporte a streaming de respostas
- **Tool calling autom√°tico**: Executa tools automaticamente durante streaming
- **Structured outputs**: Suporte a outputs estruturados

### Arquitetura: Backend vs Frontend

**Backend (Node.js/Edge/Serverless):**
```typescript
// Backend: Rota API que processa LLM
import { openai } from '@ai-sdk/openai'
import { streamText } from 'ai'

export async function POST(req) {
  const { messages } = await req.json()
  
  const result = streamText({
    model: openai('gpt-4'),
    messages,
    tools: {
      readFile: tool({
        description: 'Read a file',
        execute: async ({ path }) => {
          // Executa automaticamente quando LLM chama
          return { content: await fs.readFile(path) }
        }
      })
    }
  })
  
  // Retorna stream para frontend
  return result.toDataStreamResponse()
}
```

**Frontend (React/Vue/Svelte):**
```typescript
// Frontend: Hook React para UI
import { useChat } from '@ai-sdk/react'

function Chat() {
  const { messages, input, handleSubmit } = useChat({
    api: '/api/chat'  // Rota backend acima
  })
  
  return (
    <div>
      {messages.map(m => <div>{m.content}</div>)}
      <form onSubmit={handleSubmit}>
        <input value={input} />
      </form>
    </div>
  )
}
```

### Como Funciona a Execu√ß√£o Autom√°tica de Tools?

**O Vercel AI SDK executa tools automaticamente durante o streaming:**

```typescript
const result = streamText({
  model: openai('gpt-4'),
  messages,
  tools: {
    readFile: tool({
      description: 'Read a file',
      execute: async ({ path }) => {
        // Esta fun√ß√£o √© chamada AUTOMATICAMENTE
        // quando a LLM chama a tool durante o streaming
        return { content: await fs.readFile(path) }
      }
    })
  }
})

// Durante o streaming:
// 1. LLM gera resposta e chama readFile
// 2. SDK automaticamente executa execute()
// 3. Resultado volta para LLM
// 4. LLM continua pensando/corrigindo
// 5. Stream continua at√© LLM terminar
```

**Caracter√≠sticas:**
- ‚úÖ **Autom√°tico**: N√£o precisa aprovar cada tool
- ‚úÖ **Durante streaming**: Executa enquanto LLM gera resposta
- ‚úÖ **Resultado imediato**: LLM v√™ resultado e pode corrigir
- ‚úÖ **Loop cont√≠nuo**: Continua at√© LLM terminar naturalmente

### Suporte a Python?

**‚ùå N√£o h√° SDK oficial para Python**

**Alternativas para Python:**

1. **AI Gateway da Vercel** (recomendado):
   - Gateway que roteia para m√∫ltiplos providers
   - Usa SDKs oficiais (OpenAI, Anthropic) em Python
   - Suporta streaming e tool calling

2. **Ports da comunidade**:
   - `python-ai-sdk` (n√£o oficial)
   - Implementa√ß√µes similares em Python

3. **SDKs nativos**:
   - OpenAI SDK Python
   - Anthropic SDK Python
   - Mas sem a abstra√ß√£o unificada do AI SDK

### Suporte a React?

**‚úÖ Suporte oficial completo**

```bash
npm install @ai-sdk/react
```

**Hooks dispon√≠veis:**
- `useChat`: Para interfaces de chat
- `useCompletion`: Para completions simples
- `useAssistant`: Para assistentes com tools

### Suporte a LLMs Locais/Gratuitas?

**‚úÖ Suportado via OpenAI-compatible**

```typescript
import { createOpenAI } from '@ai-sdk/openai'

const ollama = createOpenAI({
  baseURL: 'http://localhost:11434/v1',
  apiKey: 'not-needed'  // Ollama n√£o precisa de key
})

const result = streamText({
  model: ollama('llama3.2'),
  messages,
  tools: { ... }
})
```

**Funciona com:**
- Ollama (localhost:11434)
- LM Studio (localhost:1234)
- LocalAI (qualquer porta)
- Qualquer servidor OpenAI-compatible

### Compara√ß√£o: Vercel AI SDK vs Implementa√ß√£o Pr√≥pria

| Aspecto | Vercel AI SDK | Implementa√ß√£o Pr√≥pria |
|---------|---------------|----------------------|
| **Execu√ß√£o autom√°tica** | ‚úÖ Autom√°tica durante streaming | ‚ùå Precisa implementar |
| **Tool calling** | ‚úÖ Nativo, autom√°tico | ‚ùå Precisa implementar |
| **Streaming** | ‚úÖ Nativo | ‚ùå Precisa implementar |
| **Provider-agn√≥stico** | ‚úÖ Suporta 20+ providers | ‚ùå Precisa integrar cada um |
| **Python** | ‚ùå N√£o tem | ‚úÖ Pode fazer em Python |
| **React** | ‚úÖ Hooks prontos | ‚ùå Precisa fazer do zero |
| **Complexidade** | ‚úÖ Baixa (usa lib) | ‚ùå Alta (implementa tudo) |

### Para o Forge Agent (Python + React)

**Op√ß√µes:**

1. **Manter implementa√ß√£o pr√≥pria** (atual):
   - ‚úÖ Controle total
   - ‚úÖ J√° funciona
   - ‚ùå Precisa implementar execu√ß√£o autom√°tica
   - ‚ùå Precisa implementar streaming melhor

2. **Usar AI Gateway + SDKs nativos**:
   - ‚úÖ Abstra√ß√£o de providers
   - ‚úÖ Streaming nativo
   - ‚ùå Ainda precisa implementar tool execution
   - ‚ùå Mais complexo

3. **H√≠brido: Backend Python + Frontend React com AI SDK**:
   - Backend Python: Processa LLM (OpenAI SDK, etc.)
   - Frontend React: Usa `@ai-sdk/react` para UI
   - Bridge: API REST entre eles
   - ‚úÖ Melhor UX no frontend
   - ‚úÖ Mant√©m backend Python
   - ‚ùå Precisa bridge entre Python e React

4. **Migrar backend para TypeScript/Node.js**:
   - ‚úÖ Usa Vercel AI SDK completo
   - ‚úÖ Execu√ß√£o autom√°tica nativa
   - ‚úÖ Streaming nativo
   - ‚ùå Precisa reescrever backend

### Recomenda√ß√£o para Forge Agent

**Curto Prazo:**
- Manter Python backend
- Implementar execu√ß√£o autom√°tica de tools (inspirado no AI SDK)
- Melhorar streaming

**M√©dio Prazo:**
- Considerar usar `@ai-sdk/react` no frontend
- Backend Python pode expor API compat√≠vel com AI SDK

**Longo Prazo:**
- Avaliar migra√ß√£o para TypeScript/Node.js se necess√°rio
- Ou criar port do AI SDK para Python (comunidade)

---

## üìä Compara√ß√£o Final

| Aspecto | OpenCode | Forge Agent |
|---------|----------|-------------|
| **Interface** | TUI + Desktop + Web | Web only |
| **Execu√ß√£o** | PTY (bun-pty) | Tmux |
| **Storage** | Arquivos JSON | SQLite |
| **Mem√≥ria** | Compaction + Summary | Todas mensagens |
| **Vector DB** | ‚ùå N√£o usa | ‚ùå N√£o usa (ainda) |
| **Arquitetura** | Client/Server local | API REST |
| **Tools** | Registry rico | Tools espec√≠ficos |
| **LSP** | ‚úÖ Integrado | ‚ùå N√£o tem |
| **Snapshot** | ‚úÖ Git diffs | ‚ùå N√£o tem |
| **Streaming** | ‚úÖ WebSocket | ‚ùå HTTP only |

---

## üéØ Recomenda√ß√µes para Forge Agent

### Curto Prazo
1. **Implementar Compaction**: Similar ao OpenCode
2. **Melhorar estrutura de mensagens**: Suportar parts
3. **Adicionar WebSocket**: Para streaming de output

### M√©dio Prazo
1. **Considerar PTY**: Se precisar de melhor streaming
2. **Adicionar LSP**: Para busca sem√¢ntica de c√≥digo
3. **Implementar Snapshot**: Para tracking de mudan√ßas

### Longo Prazo
1. **Vector DB opcional**: Para codebases muito grandes
2. **M√∫ltiplos clientes**: Desktop app, mobile app
3. **Skills system**: Similar ao OpenCode

---

## üèÅ Conclus√£o

OpenCode e Forge Agent s√£o projetos similares com abordagens diferentes:

- **OpenCode**: Terminal-first, focado em desenvolvedores, usa PTY, compaction autom√°tica
- **Forge Agent**: Web-first, focado em acessibilidade, usa tmux, sem compaction ainda

**Principais aprendizados:**
1. Compaction √© essencial para sess√µes longas
2. PTY vs Tmux s√£o trade-offs v√°lidos
3. Estrutura rica de mensagens ajuda muito
4. Vector DB n√£o √© necess√°rio para a maioria dos casos
5. Client/Server permite m√∫ltiplos clientes

**Recomenda√ß√£o principal**: Implementar compaction similar ao OpenCode para resolver problemas de contexto em sess√µes longas.
